{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cahya Amalinadhi's blog","text":""},{"location":"#data-science","title":"Data Science","text":"<p>Experimenting with Data.</p> <ul> <li>Modeling Count Data with Poisson Regression</li> </ul>"},{"location":"data_engineering/","title":"Data Engineering","text":"<p>Some blogs about data engineering.</p> <ul> <li>Scraping Job at Glints with Graphql</li> </ul>"},{"location":"data_engineering/scraping_job_glints/","title":"Scraping Job at Glints using Graphql","text":""},{"location":"data_engineering/scraping_job_glints/#how-to-scrap","title":"How to scrap?","text":"<pre><code>def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> bubble_sort.py<pre><code>theme:\nfeatures:\n- content.code.annotate # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>"},{"location":"data_science/","title":"Data Sciences","text":"<p>Experimenting with data.</p> <ul> <li>Modeling Count Data with Poisson Regression</li> </ul>"},{"location":"data_science/poisson_regression/","title":"Modeling Count Data with Poisson Regression","text":""},{"location":"data_science/poisson_regression/#how-to-do-the-regression","title":"How to do the regression?","text":"<p>We model the count data as</p> <p> \\( \\begin{align*} \\hat{y}_{i} = \\exp(X_{i} \\cdot \\beta) \\tag{1} \\end{align*} \\) </p>"},{"location":"data_science/poisson_regression/#how-to-optimize","title":"How to optimize?","text":"<p>We need to find the best \\(\\beta\\), thus we maximize the likelihood function. The count data can be generalized with the Poisson distribution, hence</p> <p> \\( \\begin{align*} \\mathcal{L}(\\beta)  &amp;=\\prod_{i=1}^{n}      PMF(y_{i} | X_{i}) \\\\ &amp;=\\prod_{i=1}^{n}      \\cfrac     {\\exp(-\\hat{y}_{i}) . \\hat{y}_{i}^{y_{i}}}     {y_{i}!} \\end{align*} \\) </p> <p>It's hard to optimize the \\(\\prod\\) operation, thus we \\(\\ln\\) it.</p> <p> \\( \\begin{align*} -\\ln (\\mathcal{L}(\\beta) ) &amp;= -\\sum_{i=1}^{n}     \\left (         - \\hat{y}_{i} + y_{i} . \\ln \\lambda_{i} - \\ln (y_{i}!)     \\right )\\\\ \\text{nll} (\\beta) &amp;= \\sum_{i=1}^{n}     \\left (         \\hat{y}_{i} - y_{i} . X_{i} . \\beta - \\ln (y_{i}!)     \\right )     \\tag{2} \\end{align*} \\) </p> <p>Our objective is to minimize the negative log likelihood, \\(\\text{nll}(\\beta)\\) Taking the derivative of \\(\\text{nll}(\\beta)\\) with respect to \\(\\beta\\) we get</p> <p> \\( \\begin{align*} \\cfrac{\\partial \\ \\text{nll}(\\beta)}{\\partial \\beta}  &amp;= \\sum_{i=1}^{n}     \\left (         X_{i} \\exp(X_{i}\\beta) - y_{i}X_{i}     \\right ) \\\\ &amp;= \\sum_{i=1}^{n}     \\left (         \\hat{y}_{i} - y_{i}     \\right ) X_{i} \\tag{3} \\end{align*} \\) </p> <p>We need to optimize Eq. (3) iteratively with Gradient Descent</p>"},{"location":"data_science/poisson_regression/#build-functions","title":"Build Functions","text":""},{"location":"data_science/poisson_regression/#function-to-predict-the-count","title":"Function to predict the count","text":"<p>We model the count data as</p> <p> \\( \\begin{align*} \\hat{y}_{i} = \\exp(X_{i} \\cdot \\beta) \\tag{1} \\end{align*} \\) </p> <pre><code>def predict(X, beta):\n'''Function to predict the count. Eq (1)'''\nreturn np.exp(np.dot(X, beta)) # (1)\n</code></pre> <ol> <li>We do the vectorized operations of \\(X\\) and \\(\\beta\\). This functions will return an array with shape of <code>(n_samples,)</code></li> </ol>"},{"location":"data_science/poisson_regression/#function-to-calculate-the-negative-log-likelihood","title":"Function to calculate the negative log likelihood","text":"<p>We calculate the model performance by the negative log likelihood</p> <p> \\( \\begin{align*} \\text{nll} (\\beta) &amp;= \\sum_{i=1}^{n}     \\left (         \\hat{y}_{i} - y_{i} . X_{i} . \\beta - \\ln (y_{i}!)     \\right )     \\tag{2} \\end{align*} \\) </p> <pre><code>def get_nnl(y_true, y_pred):\n'''Get the log likelihood'''\n# Get y!\ny_fact = np.array([np.math.factorial(val) for val in y_true]) # (1)\n# Get log likelihood\nnnl = np.sum(\ny_pred - y_true*np.log(y_pred) + np.log(y_fact)  # (2)\n)\nreturn nnl\n</code></pre> <ol> <li>There is no broadcasting for factorial function thus we use <code>np.math.factorial(val)</code> and list comprehension to calculate \\(\\ln (y_{i}!)\\)</li> <li>We know that \\(\\hat{y}_{i} = \\exp(X_{i} \\beta)\\), then \\(X_{i} \\beta = \\ln (\\hat{y}_{i})\\)</li> </ol>"},{"location":"data_science/poisson_regression/#modeling-example-horseshoe-crab","title":"Modeling Example - Horseshoe Crab","text":"<p>We want to model the count of Horseshoe crab satellites. We get the data from here.</p> <pre><code># Run this first\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n</code></pre>"},{"location":"data_science/poisson_regression/#load-the-data","title":"Load the data","text":"<p>Save the data to an excel format, then load the data <pre><code># Create a function to read data\ndef read_data(fname):\n\"\"\"Function to read excel data &amp; return as Pandas DataFrame\"\"\"\ndata = pd.read_excel(fname)\nprint('Data shape:', data.shape)\nreturn data\n# Read the data\ndata = read_data(fname = 'horseshoe crab.xlsx')\nprint(data.head())\n</code></pre> <pre><code>Data shape: (173, 5)\n   Color  Spine  Width  Weight  Satellite\n0      2      3   28.3    3.05          8\n1      3      3   26.0    2.60          4\n2      3      3   25.6    2.15          0\n3      4      2   21.0    1.85          0\n4      2      3   29.0    3.00          1\n</code></pre></p> <p>There are 173 number of samples with 5 columns.</p>"},{"location":"data_science/poisson_regression/#what-we-want-to-predict","title":"What we want to predict","text":"<p>We predict the count of <code>Satellite</code> in Horseshoe crabs.</p> <p><pre><code># Create a function to read data\nsns.histplot(x = 'Satellite',\nbins = 15,\ndata = data)\nplt.show();\n</code></pre> </p>"},{"location":"data_science/poisson_regression/#split-input-output-data","title":"Split input &amp; output data","text":"<p>First, create a function to split input &amp; output</p> <pre><code># Split data\ndef split_input_output(data, input_cols, target_col):\n\"\"\"Split input-output\"\"\"\nX = data[input_cols]\ny = data[target_col]\nprint('X shape:', X.shape)\nprint('y shape:', y.shape)\nreturn X, y\n# Split data\nX, y = split_input_output(data = data,\ninput_cols = ['Width'], # (1)\ntarget_col = 'Satellite')\n</code></pre> <ol> <li>For simplicity, we only choose one prediction, i.e. <code>Width</code></li> </ol> <pre><code>X shape: (173, 1)\ny shape: (173,)\n</code></pre> <p>Print the splitting results <pre><code>print(X.head())\n</code></pre> <pre><code>   Width\n0   28.3\n1   26.0\n2   25.6\n3   21.0\n4   29.0\n</code></pre> <pre><code>print(y.head())\n</code></pre> <pre><code>0    8\n1    4\n2    0\n3    0\n4    1\nName: Satellite, dtype: int64\n</code></pre></p>"},{"location":"data_science/poisson_regression/#prepare-for-the-optimization","title":"Prepare for the optimization","text":"<p>We want to optimize \\(\\beta\\). Recall the Equation (1)</p> <p> \\( \\begin{align*} \\hat{y}_{i} &amp;= \\exp(X_{i} \\cdot \\beta) \\tag{1} \\\\ \\end{align*} \\) </p> <p>In a vectorized form, we have</p> <p> \\( \\begin{align*} \\begin{bmatrix} \\hat{y}_{1}\\\\  \\hat{y}_{2}\\\\  \\vdots\\\\ \\hat{y}_{n} \\end{bmatrix} &amp;=  \\exp \\left(     \\begin{bmatrix}     \\beta_{0} + \\beta_{1}x_{1}\\\\      \\beta_{0} + \\beta_{1}x_{2}\\\\      \\vdots\\\\     \\beta_{0} + \\beta_{1}x_{n}     \\end{bmatrix} \\right ) \\\\ \\\\ \\mathbf{\\hat{y}}  &amp;= \\exp \\left(     \\begin{bmatrix}     1 &amp; x_{1}\\\\      1 &amp; x_{2}\\\\      \\vdots &amp; \\vdots\\\\     1 &amp; x_{n}     \\end{bmatrix}     \\cdot     \\begin{bmatrix}     \\beta_{0}\\\\      \\beta_{1}     \\end{bmatrix} \\right ) \\\\ \\\\ \\mathbf{\\hat{y}} &amp;= \\exp (\\mathbf{A} \\cdot \\mathbf{\\beta}) \\end{align*} \\) </p> <p>We have to recreate \\(\\mathbf{A}\\) first, <pre><code># Extract data shape\nn = X.shape[0]\nprint(n)\n</code></pre> <pre><code>173\n</code></pre> <pre><code># Create design matrix\nA = np.column_stack((np.ones(n), X))\nprint(A[:10])\n</code></pre> <pre><code>[[ 1.  28.3]\n [ 1.  26. ]\n [ 1.  25.6]\n [ 1.  21. ]\n [ 1.  29. ]\n [ 1.  25. ]\n [ 1.  26.2]\n [ 1.  24.9]\n [ 1.  25.7]\n [ 1.  27.5]]\n</code></pre></p> <p>We will use \\(\\mathbf{A}\\) and \\(\\mathbf{y}\\) during the optimization process.</p> <p>Next, we have to initialize the model parameters \\(\\beta\\) <pre><code># Initialize the model parameter\n# [beta_0, beta_1]\nbeta = np.array([-y.mean(), 0.0])\nprint(beta)\n</code></pre> <pre><code>[-2.91907514  0.        ]\n</code></pre></p>"},{"location":"data_science/poisson_regression/#the-gradient-descent","title":"The Gradient Descent","text":"<p>Our objective is to minimize the negative log likelihood, hence we update the \\(\\beta\\):</p> <p> \\( \\beta^{(i+1)} := \\beta^{(i)} - \\text{lr} \\cdot \\cfrac{\\partial \\ \\text{nll}(\\beta)}{\\partial \\beta} \\tag{4} \\) </p> <p>We do this iteratively, thus</p> <pre><code># Do the optimization\nmax_iter = 500_000    # (1)\nlearning_rate = 5e-6  # (2)\nfor i in range(max_iter):\n# Find prediction\ny_pred = predict(A, beta) # (3)\n# Find likeliheood\nnll = get_nll(y, y_pred) # (4)\n# Find grad\ngrads = np.dot(A.T, (y_pred - y)) # (5)\n# Update\nbeta += -learning_rate * grads # (6)\n# Print\nif i%10_000==0 or i==0:\nprint(f'iter:{i+1}, nll: {nll:.4f}, beta:{np.round(beta,4)}') # (7)\n</code></pre> <ol> <li> <p>This is a stopping criterion. When <code>i == max_iter-1</code> the optimization process will stop.</p> </li> <li> <p><code>learning_rate</code> will control how big the changes is when updating the model parameter during optimization process. A big value of <code>learning_rate</code> can make the model parameter updates diverges.</p> </li> <li> <p>Predict the count using Eq. (1)</p> </li> <li> <p>Predict the negative log likelihood using Eq. (2)</p> </li> <li> <p>Predict the gradient of NLL using Eq. (3). We use vectorized operation, i.e. <code>np.dot(A, B)</code>, instead of <code>np.sum(A * B)</code>.</p> </li> <li> <p>Update the model parameter using Eq. (4)</p> </li> <li> <p>Print the optimization process. You can monitor the NLL. There's something wrong if your NLL is getting bigger.</p> </li> </ol> <pre><code>iter:1, nll: 2013.5065, beta:[-2.9166  0.0671]\niter:10001, nll: 461.7734, beta:[-2.9746  0.1519]\niter:20001, nll: 461.7201, beta:[-3.0261  0.1538]\niter:30001, nll: 461.6821, beta:[-3.0696  0.1554]\niter:40001, nll: 461.6551, beta:[-3.1063  0.1568]\niter:50001, nll: 461.6358, beta:[-3.1373  0.1579]\n...\niter:450001, nll: 461.5881, beta:[-3.3046  0.164 ]\niter:460001, nll: 461.5881, beta:[-3.3046  0.164 ]\niter:470001, nll: 461.5881, beta:[-3.3046  0.164 ]\niter:480001, nll: 461.5881, beta:[-3.3046  0.164 ]\niter:490001, nll: 461.5881, beta:[-3.3047  0.164 ]\n</code></pre> <p>Finally, we get our trained model parameter \\(\\hat{\\beta}\\) <pre><code># Final model parameter\nprint(beta)\n</code></pre> <pre><code>[-3.30467683  0.16404214]\n</code></pre></p>"},{"location":"data_science/poisson_regression/#evaluation","title":"Evaluation","text":"<p>Let's check its performance. First, we make a prediction</p> <pre><code># Create a prediction\ny_pred = predict(A, beta)\nprint(y_pred)\n</code></pre> <pre><code>[3.81032936 2.61279026 2.44685093 1.15051312 4.2739785  2.21749145\n2.6999335  2.18141199 2.48732062 3.34170756 2.65600451 4.20443917\n...\n3.62735231 3.87335031 8.9417806  2.52845967 1.88199888 1.62368218\n3.81032936 2.83612812 2.83612812 2.65600451 2.04286965] # (1)\n</code></pre> <ol> <li>The results of Poisson model is not <code>int</code></li> </ol> <p>We cannot compare anything right now, let's plot the count distribution of <code>Satellite</code> between actual &amp; predicted value.</p> <p><pre><code># Create visualization\nsns.histplot(y, color='b', bins=8, alpha=0.3, label='actual')\nsns.histplot(y_pred, color='r', bins=8, alpha=0.3, label='pred')\nplt.legend()\nplt.show();\n</code></pre> </p> <p>Not a good prediction though.</p>"},{"location":"data_science/poisson_regression/#modeling-with-statsmodel","title":"Modeling with Statsmodel","text":"<p>Now, let's try modeling with a well-known library: <code>statsmodels</code></p> <pre><code>import statsmodels.api as sm\n</code></pre> <p>Do the modeling</p> <pre><code># Create the object\npoi_reg = sm.GLM(y, A,\nfamily = sm.families.Poisson()) # (1)\n# Fit the object\npoi_res = poi_reg.fit()\n</code></pre> <ol> <li>Poisson regression is a family of GLM (generalized linear model).</li> </ol> <p>Then print the results</p> <p><pre><code># Print results\nprint(poi_res.summary())\n</code></pre> <pre><code>                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:              Satellite   No. Observations:                  173\nModel:                            GLM   Df Residuals:                      171\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -461.59\nDate:                Sat, 17 Jun 2023   Deviance:                       567.88\nTime:                        14:28:46   Pearson chi2:                     544.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.3129\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -3.3048      0.542     -6.095      0.000      -4.368      -2.242\nx1             0.1640      0.020      8.216      0.000       0.125       0.203\n==============================================================================\n</code></pre></p> <p>As we can see, the coefficient is <code>[-3.3048, 0.1640]</code> similar to our from scratch results.</p> <p>Eventhough the results is not good enough, however we can validate that our from scratch works fine compared to the <code>statsmodels</code> library.</p>"}]}